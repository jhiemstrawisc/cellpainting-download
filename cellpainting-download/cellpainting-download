#!/usr/bin/python3

import argparse
import http.client
import os
import pathlib
import shutil
import subprocess
import sys
import tempfile
import time
import zipfile

import classad
import htcondor
import htcondor.dags


class ExistingWorkingDir(Exception):
    pass


def generateDAG(prefix, working_dir, max_running, max_measurements, destination_dir, measurement_file):

    measurements = []
    with open(measurement_file, "r") as fp:
        for line in fp:
            measurements.append(line.strip())
            if max_measurements and len(measurements) >= max_measurements:
                break

    script_path = str(pathlib.Path(sys.argv[0]).absolute())
    mc_path = str(( pathlib.Path(sys.argv[0]).parent / "mc" ).absolute())
    downloader_submit_description = htcondor.Submit({
        "executable":              script_path,
        "arguments":              f"exec $(MEASUREMENT) {destination_dir}",
        "universe":                "vanilla",
        "request_disk":            "100GB",
        "request_cpus":             1,
        "request_memory":           512,
        "log":                    f"{prefix}-$(CLUSTER).log",
        "should_transfer_files":   "YES",
        "when_to_transfer_output": "ON_EXIT",
        # Hack to target only the tech refresh hosts (which have significantly more network capacity)
        "requirements":            'CpuModel =?= "AMD EPYC 7763 64-Core Processor"',
        "transfer_input_files":    mc_path,
        "transfer_output_files":   "config/config.json",
        "output":                  "download-$(JOB)_$(RETRY).out",
        "error":                   "download-$(JOB)_$(RETRY).err",
    })
    working_dir_path = pathlib.Path(working_dir)

    dag = htcondor.dags.DAG()
    dag.layer(
       name = prefix,
       submit_description = downloader_submit_description,
       vars = [{"node_name": f"Measurement-{idx}", "MEASUREMENT": measurements[idx]} for idx in range(len(measurements))],
       retries = int(3),
    )

    dag_dir = pathlib.Path(working_dir).absolute()
    try:
        dag_dir.mkdir()
    except FileExistsError:
        dir_str = str(dag_dir)
        raise ExistingWorkingDir(f"Working directory, {dir_str}, already exists; remove to reuse")

    dag_file = htcondor.dags.write_dag(dag, dag_dir, node_name_formatter=htcondor.dags.SimpleFormatter("_"))

    dag_submit = htcondor.Submit.from_dag(str(dag_file),{
        'batch-name': prefix,
        'maxjobs': max_running
    })

    os.chdir(dag_dir)
    schedd = htcondor.Schedd()
    submit_result = schedd.submit(dag_submit)
    print("Download jobs were submitted as DAG with JobID %d.0" % submit_result.cluster())


def countDags(prefix):
    schedd = htcondor.Schedd()
    return len(list(schedd.query(constraint='JobBatchName =?= %s' % classad.quote(prefix), projection=[])))


def writeMCConfig():
    try:
        os.mkdir("config")
    except FileExistsError:
        pass
    with open("config/config.json", "w") as fp:
        fp.write("""
{
        "version": "10",
        "aliases": {
                "s3": {
                        "url": "https://s3.dualstack.us-east-1.amazonaws.com",
                        "api": "S3v4",
                        "path": "off"
                }
        }
}
""")


def helperMain():
    parser = argparse.ArgumentParser(description="Run a script for the cellpainting download application")
    parser.add_argument("command", help="Helper command to run", choices=["exec"])
    parser.add_argument("measurement", help="Name of the measurement collection to download")
    parser.add_argument("destination", help="Mounted destination path for resulting zipfile")

    args = parser.parse_args()

    zipname = args.measurement.replace("/", "_") + ".zip"
    output_file = os.path.join(args.destination, zipname)
    if os.path.exists(output_file):
        return 0

    writeMCConfig()

    subprocess.run(["./mc", "--config-dir", "config", "mirror", "s3/" + args.measurement, args.measurement], env={"PATH": "/usr/bin"}, check=True)

    #subprocess.run(["/usr/bin/zip", "-r0", zipname, args.measurement.split("/")[0]], check=True)
    with zipfile.ZipFile(zipname, mode='w') as zf:
        for dirpath, dirnames, filenames in os.walk(args.measurement):
            for filename in filenames:
                full_filename = os.path.join(dirpath, filename)
                zf.write(full_filename)
                os.unlink(full_filename)

    subprocess.run(["/usr/bin/mv", zipname, output_file], check=True)
    # shutil.copy(zipname, os.path.join(args.destination, zipname))


def topMain():

    parser = argparse.ArgumentParser(description="Execute a download of a cellpainting dataset")
    parser.add_argument("command", help="Sub-command to run", choices=["submit", "resubmit", "list"])
    parser.add_argument("--instance", help="Instance name for the run", default="cellpainting-download")
    parser.add_argument("-w", "--working-dir", help="Working directory for the DAG associated with the download instance", default="working_dir")
    parser.add_argument("-m", "--measurements", help="File containing list of all the measurements to download", default="measurements.txt")
    parser.add_argument("-d", "--destination", help="Destination directory for output zipfiles", required=True)
    parser.add_argument("-r", "--max-running", help="Maximum number of running download jobs", default=5)
    parser.add_argument("--max-measurements", help="Maximum number of measurements to download", type=int)

    args = parser.parse_args()

    if countDags(args.instance):
        print(f"Cannot submit new download named {args.instance}; one already exists in queue. To create a new instance, use the --instance option and provide a new name.")
        return 2

    if args.command != "submit":
        print(f"Command {args.command} is not implemented")
        return 3

    generateDAG(args.instance, args.working_dir, args.max_running, args.max_measurements, args.destination, args.measurements)
    return 0


def main():
    # The same script serves as both the driver and the EP-side wrapper. Look
    # at argv[1] to see what we should do in order to avoid dumping confusing help
    # options to the user
    if len(sys.argv) > 1 and sys.argv[1] in ["exec"]:
        return helperMain()

    return topMain()

if __name__ == '__main__':
    sys.exit(main())
